{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Attendance to a Test Preparation Course Based on Candidates' Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test preparation courses are a form of shadow education, which is referred to as \"educational activities, such as tutoring and extra classes,\n",
    "occurring outside of the formal channels of an educational system\" (Buchmann et al., 436) and are used with the intention of increasing students' chances of success in high school courses and gaining admission into the post-secondary institute of their choice. A few companies offering these courses are confident their services are effective, and go as far as to offer a return of clients' money if a high score is not achieved (Buchmann et al., 440). \n",
    "\n",
    "Predictive Question: Can we use the exam scores of students to predict whether they attended a test preparation course?\n",
    "\n",
    "The `all_exams.csv` data set is used to determine whether a student took a test prep course. Their exam scores from math, reading, and writing (**Note that when we start Performing KNN Classification we only used math and reading exam scores as our predictors because more then that was causing the kernal to die and analysis to be slow**)  would identify if they attended a test prep course. The data set also contains information about high school students from the US, and includes the students’ gender, race/ethnicity, parental level of education, and lunch access. The size of the sample was increased to 1200 by combining the downloaded data, since the data is generated spontaneously. By doing this, we expect our model to have a higher accuracy because it will be able to gain familiarity with more data examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "library(RColorBrewer)\n",
    "library(GGally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows = 10)\n",
    "all_exams<-read_csv(\"https://raw.githubusercontent.com/SopTes27/group26_project/main/GP_data/all_exams.csv\")\n",
    "all_exams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the X1 column that will not be used in our model from the original data set. Then, we make the gender, race/ethnicity, parental level of education, lunch, and test preparation course columns as category data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(all_exams)<-c(\"X1\", \"gender\", \"race_ethnicity\", \"parental_level_of_education\",\n",
    "\"lunch\", \"test_preparation_course\", \"math_score\", \"reading_score\", \"writing_score\")\n",
    "\n",
    "tidying_data <-select(all_exams, gender:writing_score)%>%\n",
    "    mutate(across(gender:test_preparation_course, as.factor))\n",
    "tidying_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `tidying_data` dataset created in the previous step, create a new column in the dataset called `avg_grade` by grouping the test_preparation_course, math_score, reading_score, and writing_score and calculating the average grade. The new dataset created is named `exams_data`. The new average grade column represents the mean of students' combined math, reading, and writing scores. The average grade will be used as a predictor in the data analysis performed later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_data<-tidying_data %>%\n",
    "    rowwise(math_score:writing_score)%>%\n",
    "    mutate(avg_grade=mean(math_score:writing_score))%>%\n",
    "    select(test_preparation_course, math_score, reading_score, writing_score, avg_grade)\n",
    "exams_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exams_data` dataset is split into a training set and a testing set. The training set will contain 75% of the dataset, and be named `exam_train`. The testing set will contain 25% of the data from `exams_data`, and will be named `exam_test`. The seed is also set to 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2021)\n",
    "\n",
    "data_split <- initial_split(exams_data, prop = 0.75, strata = test_preparation_course)\n",
    "exam_train <- training(data_split)\n",
    "exam_test <- testing(data_split)\n",
    "\n",
    "glimpse(exam_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis - Creating a Summary and Visualization of the `exams_data` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the training and testing datasets were examined for any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(is.na(exam_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(is.na(exam_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check the number of observations in both the training and testing datasets. This is performed to determine whether there is a class imbalance present in the data before upsampling. From Table ?? and Table ?? below, we can conclude that there is a class imbalance present in the training data, because students who did not take the test preparation course were more common than those who did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_train <- nrow(exam_train)\n",
    "exam_train %>%\n",
    "  group_by(test_preparation_course) %>%\n",
    "  summarize(\n",
    "    count = n(), \n",
    "    percentage = n() / num_obs_train \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_test <- nrow(exam_test)\n",
    "exam_test %>%\n",
    "    group_by(test_preparation_course)%>%\n",
    "    summarize(\n",
    "        count = n(), \n",
    "        percentage = n() / num_obs_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the class imbalance in the training data, upsampling is conducted on only the training dataset to balance the data, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_recipe <- recipe(test_preparation_course ~ ., data = exam_train)%>% \n",
    "  step_upsample(test_preparation_course, over_ratio = 1, skip = FALSE)%>%\n",
    "  prep() \n",
    "exam_recipe\n",
    "\n",
    "upsampled_exam <- bake(exam_recipe, exam_train)\n",
    "\n",
    "upsampled_exam %>%\n",
    "  group_by(test_preparation_course) %>%\n",
    "  summarize(n = n())\n",
    "upsampled_exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 7 below summarizes the values of the predictor variables in the training set which will be used later on in our data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_means <- exam_train%>%\n",
    "    group_by(test_preparation_course)%>%\n",
    "    summarize(\n",
    "        math_score_average=mean(math_score),\n",
    "        writing_score_average=mean(writing_score),\n",
    "        reading_score_average=mean(reading_score),\n",
    "        total_average_score=mean(avg_grade)\n",
    "    )\n",
    "predictor_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Table 8 summarizes all of the data present in the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(exam_train) \n",
    "do.call(cbind, lapply(exam_train, summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of the exploratory data analysis was to create a visualization representing the relationship that each predictor variable had with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 15, repr.plot.height = 20) \n",
    "\n",
    "bar_legend<-grab_legend(predictor_plots)\n",
    "\n",
    "Pairwise_Matrix_legend<- ggpairs(exam_train, title = \"Pairwise Matrix Plot\", legend = bar_legend,\n",
    "                           aes(alpha = 0.2, color = test_preparation_course))+\n",
    "labs(fill=\"Test Preparation Course\")+\n",
    "exam_recipe <- recipe(test_preparation_course ~math_score+ reading_score, data = exam_train) %>%\n",
    "                step_scale(all_predictors()) %>%\n",
    "                step_center(all_predictors())\n",
    "exam_recipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis - Performing KNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before scaling and centering the data, a 5-fold cross-validation is performed to tune the hyperparameters. The strata argument is set as our categorical target variable, which is the `test_preparation_course`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_vfold <- vfold_cv(exam_train, v = 5, strata = test_preparation_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our KNN classification model, we will first create a recipe using the training data. The recipe specifies the target variable (test_preparation_course) and the predictors, and also scales and centers the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Having more the 2 predictor varibles was causing the kernal to die and making the code run for a long time. So we decided to use just `math_score`  `reading_score` as our predictor varibles while preforming KNN classification. This will also make it easier to visulize results.\n",
    "\n",
    "You can't just choose whichever predictors you want. If you want only two, then we need to choose writing score, and average score because if you look at the distribution plots, those are the two with the most divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exam_recipe <- recipe(test_preparation_course ~ ., data = exam_train) %>%\n",
    "#                 step_scale(all_predictors()) %>%\n",
    "#                 step_center(all_predictors())\n",
    "# exam_recipe\n",
    "\n",
    "exam_recipe <- recipe(test_preparation_course ~math_score+ reading_score, data = exam_train) %>%\n",
    "                step_scale(all_predictors()) %>%\n",
    "                step_center(all_predictors())\n",
    "exam_recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we created the K-nearest neighbors classifier and tuned each parameter in the model. In the next code block, cross validation is used to evaluate the the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "       set_engine(\"kknn\") %>%\n",
    "       set_mode(\"classification\")\n",
    "knn_tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we created a dataframe named `k_vals` that has a sequence of K values between 1 and 20 we would like to test out. This new argument is passed through the grid argument of the `tune_grid` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals <- tibble(neighbors = seq(from = 1, to = 50))\n",
    "knn_results <- workflow() %>%\n",
    "       add_recipe(exam_recipe) %>%\n",
    "       add_model(knn_tune) %>%\n",
    "       tune_grid(resamples = exam_vfold, grid = k_vals) %>%\n",
    "       collect_metrics()\n",
    "\n",
    "accuracies <- knn_results %>% \n",
    "       filter(.metric == \"accuracy\" )\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step of our KNN classification model, we plotted a visualization of the accuracy versus K value to deduce which K value would be the best. From the plot below, k = 28 would be the best value because it has the highest accuracy on the graph, and we can see that values neighbouring 28 do not have any dramatic changes in accuracy . And we confirmed that by arranging the means of the k values from greatest to smallest confirming the k=28 has the greatest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=15, repr.plot.height=15)\n",
    "accuracy_versus_k <- ggplot(accuracies, aes(x = neighbors, y = mean))+\n",
    "       geom_point() +\n",
    "       geom_line() +\n",
    "       labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "       scale_x_continuous(breaks = seq(0, 50, by = 1)) +  \n",
    "       scale_y_continuous(limits = c(0.4, 1.0)) +\n",
    "ggtitle(\"Figure 2: Accuracies of K\")\n",
    "accuracy_versus_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k<-accuracies%>%arrange(desc(mean)) \n",
    "head(best_k)\n",
    "\n",
    "# what is this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 28) %>%\n",
    "  set_engine(\"kknn\") %>%\n",
    "  set_mode(\"classification\")\n",
    "knn_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit <- knn_spec %>%\n",
    "  fit(test_preparation_course ~writing_score+ avg_grade , data = exam_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize our data analysis, we plotted a decision boundary graphs. `Decision Boundaries` graph, has the math score on the x-axis versus the reading score on the y-axis. The blue and organe clouds indicate what classification a point would be given if it were to lie within it.  **This visualizations can be used to...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math_grid <- seq(min(exam_train $ math_score), \n",
    "#                 max(exam_train $ math_score), \n",
    "#                 length.out = 100)\n",
    "wri_grid <- seq(min(exam_train $ writing_score), \n",
    "                max(exam_train $ writing_score), \n",
    "                length.out = 100)\n",
    "# read_grid <- seq(min(exam_train $ reading_score), \n",
    "#                 max(exam_train $ reading_score), \n",
    "#                 length.out = 100)\n",
    "avg_grid <- seq(min(exam_train$avg_grade), \n",
    "                max(exam_train$avg_grade), \n",
    "                length.out = 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asgrid <- as_tibble(expand.grid(#math_score = math_grid, \n",
    "                                writing_score = wri_grid,\n",
    "                                #reading_score = read_grid\n",
    "                                avg_grade = avg_grid\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#double check the code \n",
    "knnPredGrid <- predict(knn_fit, asgrid)%>%\n",
    "bind_cols(asgrid)%>%\n",
    "rename(test_preparation_course = .pred_class)\n",
    "knnPredGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=15, repr.plot.height=13)\n",
    "wkflw_plot <-\n",
    "  ggplot() +\n",
    "  geom_point(data = exam_train, \n",
    "             mapping = aes(x = writing_score, \n",
    "                           y = avg_grade, \n",
    "                           color = test_preparation_course), \n",
    "             alpha = 0.75) +\n",
    "  geom_point(data = knnPredGrid, \n",
    "             mapping = aes(x = writing_score, \n",
    "                           y = avg_grade, \n",
    "                           color = test_preparation_course), \n",
    "             alpha = 0.1, \n",
    "             size = 5) +\n",
    "  labs(color = \"Attendance to Test Preparation Course\", \n",
    "       x = \"Writing Scores (%)\", \n",
    "       y = \"Average Grades (%)\") +\n",
    "  scale_color_manual(labels = c(\"Completed\", \"Not Completed\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\"))+\n",
    "ggtitle(\"Figure 3: Decision Boundaries Displaying the Relationship Between the Writing and Average Scores\")+\n",
    "theme(text = element_text(size = 15))\n",
    "wkflw_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Decision Boundary` graph shows the relationships that exist between our predictor values. As seen above, students who have completed the test preparation course tend to have scored higher than those who have not. This is especially notable in the students reading scores. As you can see the model tends to predict the test prep course as completed when a point lies in the upper left side of the graph. As we saw before there is more distribution in the math score and test prep course attendance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted whether a student attended a test prep course based on their math, reading, writing and average scores. The conclusion drawn from our data analysis was that… ☹ \n",
    "\n",
    "Based on previous studies on the topic of test preparation scores, it has been shown that students who had attended test preparation courses received higher scores than those who studied independently (Buchmann et al., 450). Although the increase in scores was not significantly high, it was noticeable enough to improve students’ chances of being admitted into their choice of college (Buchmann et al., 450). This information led us to expect a correlation between high exam scores and the completion of test preparation scores.  \n",
    "\n",
    "The information extracted from this data analysis is important in determining the effectiveness of the test preparation course in students’ performance. Based on the results of this analysis, future projects could examine the impact of the test preparation courses compared to self-studying methods in students. Other factors that have not been considered in this data set could also be explored. For example, it has been shown that the taking test preparation courses in certain years may be more effective than others when studying for college exams (Devine-Eller, 475). Future studies may be interested in determining the potential benefits and detriments to attending test preparation studies at different periods of a student’s high school career. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alon, S. \"Commentaries: Racial Differences in Test Preparation Strategies: A Commentary on Shadow Education, American      Style: Test Preparation, the SAT and College Enrollment.\" *Social Forces*, vol. 89, no. 2, 2010, pp. 463-474.\n",
    "\n",
    "Devine-Eller, Audrey. “Timing Matters: Test Preparation, Race, and Grade Level.” *Sociological Forum*, vol. 27, no. 2, [Wiley, Springer], 2012, pp. 458–80, http://www.jstor.org/stable/23262117.\n",
    "\n",
    "Kimmons, Royce. “Exams Scores For Students at a Public School.” *Exam Scores*, http://roycekimmons.com/tools/generated_data/exams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k<-accuracies%>%arrange(desc(mean)) \n",
    "head(best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when picking themost acurate k why was 5 choosen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**if having a bad graph we can justify it by us picking a not the most accuate k.**\n",
    "\n",
    "accuracy is highest of all the k you tried and accuracy is stable so if k goes up or down by one there isn't a drastic change the accuracy is roughly in the same ballpark of numbers for neugbouring k values.\n",
    "\n",
    "look at k=5 there is some flux in the accuracy of the k values next to it maybe we should have pick k=10? \n",
    "\n",
    "k = 5 is definitely our best k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to make decision boundaries visulization of our model we need to use the `contour` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
